{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZATION\n",
    "# for initializing the model parameters in training\n",
    "from torch.nn.init import xavier_normal # alternative initializations can be found at torch.nn.init\n",
    "# CONFIGURATIONS\n",
    "import config\n",
    "# UTILITIES\n",
    "from utils.dataset import CNNDMdataset\n",
    "from utils.masks import create_mask\n",
    "from utils.transforms import construct_token_transform, construct_vocab_transform, construct_tensor_transform, construct_text_transform\n",
    "# DECODERS\n",
    "from decoders.greedy import greedy_decode\n",
    "from decoders.beamsearch import beam_search_decode\n",
    "from decoders.temperature import temperature_decode\n",
    "from decoders.mbrd import mbr_decode, mbr_decode_verbose, candidates_set\n",
    "from decoders.uniform import uniform_decode\n",
    "# TRANSFORMER MODEL\n",
    "from EncoderDecoderTransformer import EncoderDecoderTransformer\n",
    "# EVALUATION\n",
    "from evaluation.rouge_evaluator import RougeEvaluator\n",
    "\n",
    "\n",
    "# PYTORCH\n",
    "import torch\n",
    "# TRACKING PROGRESS\n",
    "from timeit import default_timer as timer\n",
    "import tqdm\n",
    "import time\n",
    "# DATA MANAGEMENT\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abstractive_summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# CNNDM Datasets\n",
    "eval_PATH = 'data/test.json'\n",
    "train_PATH = 'data/train.json'\n",
    "val_PATH = 'data/validation.json'\n",
    "\n",
    "# trim the data for quick checks on laptop\n",
    "e_trim = 10 # 10000\n",
    "t_trim = 50 # 10000\n",
    "v_trim = 20 # 10000\n",
    "\n",
    "with open(eval_PATH, 'r') as f:\n",
    "  eval_data = json.load(f)[:e_trim]\n",
    "with open(train_PATH, 'r') as f:\n",
    "  train_data = json.load(f)[:t_trim]\n",
    "with open(val_PATH, 'r') as f:\n",
    "  val_data = json.load(f)[:v_trim]\n",
    "\n",
    "train_articles = [article['article'] for article in train_data]\n",
    "train_summaries = [article['summary'] for article in train_data]\n",
    "val_articles = [article['article'] for article in val_data]\n",
    "val_summaries = [article['summary'] for article in val_data]\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "eval_summaries = [article['summary'] for article in eval_data]\n",
    "\n",
    "# Remove the outliers from the Training Data\n",
    "# t_remove = [1469,1814]\n",
    "# len(train_articles[1469]), len(train_summaries[1469])\n",
    "# len(train_articles[1814]), len(train_summaries[1814])\n",
    "# del train_articles[1469]\n",
    "# del train_summaries[1469]\n",
    "# del train_articles[1814]\n",
    "# del train_summaries[1814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = abstractive_summarizer.AbstractiveSummarizer()\n",
    "summarizer.train(train_articles, train_summaries, val_articles, val_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_decode_T = lambda T: lambda summarizer_in, src_in, src_mask_in, max_len_in, start_symbol_in: temperature_decode(summarizer_in, src_in, src_mask_in, max_len_in, start_symbol_in, temperature=T)\n",
    "evaluator = RougeEvaluator()\n",
    "decoders = {'Random (Uniform)': uniform_decode, \n",
    "            'Temperature 0.25': temperature_decode_T(0.25),\n",
    "            'Temperature 0.5': temperature_decode_T(0.5),\n",
    "            'Temperature 0.75': temperature_decode_T(0.75),\n",
    "            'Temperature 1 (Unbiased)': temperature_decode_T(1),\n",
    "            'Temperature 1.25': temperature_decode_T(1.25),\n",
    "            'Temperature 1.5': temperature_decode_T(1.5),\n",
    "            'Temperature 1.75': temperature_decode_T(1.75),\n",
    "            'Temperature 2': temperature_decode_T(2),\n",
    "            'Temperature 5': temperature_decode_T(5),\n",
    "            'Temperature 10': temperature_decode_T(10),\n",
    "            'Beam Search' : beam_search_decode,\n",
    "            'Greedy': greedy_decode,\n",
    "            'MBR' : mbr_decode\n",
    "            }\n",
    "pred_summaries_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_name = 'Greedy'\n",
    "predictor = summarizer.predict(eval_articles, decoder=decoders[decoder_name])\n",
    "pred_summaries = [s for s in predictor]\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "eval_out_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, pred_summaries)]\n",
    "pred_summaries_dict[decoder_name] = eval_out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_name = 'Temperature 0.5'\n",
    "predictor = summarizer.predict(eval_articles, decoder=decoders[decoder_name])\n",
    "pred_summaries = [s for s in predictor]\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "eval_out_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, pred_summaries)]\n",
    "pred_summaries_dict[decoder_name] = eval_out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_name = 'Temperature 1 (Unbiased)'\n",
    "predictor = summarizer.predict(eval_articles, decoder=decoders[decoder_name])\n",
    "pred_summaries = [s for s in predictor]\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "eval_out_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, pred_summaries)]\n",
    "pred_summaries_dict[decoder_name] = eval_out_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_summaries_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write nested JSON data to a file\n",
    "with open('data_pred/pred.json', 'w') as f:\n",
    "    json.dump(pred_summaries_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "epochs = range(len(summarizer.train_losses))\n",
    "train_losses = summarizer.train_losses\n",
    "val_losses = summarizer.val_losses\n",
    "\n",
    "# Improved plot\n",
    "plt.figure(figsize=(10, 6))  # Larger figure size\n",
    "plt.plot(epochs, train_losses, label='Training Loss', color='blue', linewidth=2, marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', color='red', linewidth=2, marker='x')\n",
    "\n",
    "plt.title('Cross-Entropy Loss across Epochs', fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('Cross-Entropy Loss', fontsize=12)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)  # Add grid for better readability\n",
    "plt.tight_layout()  # Adjusts the plot to fit into the figure area.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer.model_paths"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
